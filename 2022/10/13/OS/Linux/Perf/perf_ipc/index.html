<!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
	
	<!-- title -->
	
	<title>
	
		Perf IPC以及CPU利用率 | 
	 
	LeahGe
	</title>
	
	<!-- keywords,description -->
	 

	<!-- favicon -->
	
	<link rel="shortcut icon" href="/favicon.ico">
	


	<!-- search -->
	<script>
		var searchEngine = "https://www.google.com/search?q=";
		if(typeof searchEngine == "undefined" || searchEngine == null || searchEngine == ""){
			searchEngine = "https://www.google.com/search?q=";
		}
		var homeHost = "leahge.github.io";
		if(typeof homeHost == "undefined" || homeHost == null || homeHost == ""){
			homeHost = window.location.host;
		}
	</script>


	
<link rel="stylesheet" href="/css/main.css">

	
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/darcula.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">


	
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery-pjax@2.0.1/jquery.pjax.min.js"></script>

	
<script src="/js/main.js"></script>

	
		
<script src="https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min.js"></script>

		
<script src="https://cdn.jsdelivr.net/npm/valine@v1.4.14/dist/Valine.min.js"></script>

	
	
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
	<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?3efe99c287df5a1d6f0d02d187e403c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<header id="header">
    <a id="title" href="/" class="logo">LeahGe</a>

	<ul id="menu">
		<li class="menu-item">
			<a href="/about" class="menu-item-link">ABOUT</a>
		</li>
	

	

		<li class="menu-item">
			<a href="https://github.com/leahge" class="menu-item-link" target="_blank">
				<i class="fa fa-github fa-2x"></i>
			</a>
		</li>
	</ul>
</header>

	
<div id="sidebar">
	<button id="sidebar-toggle" class="toggle" ><i class="fa fa-arrow-right " aria-hidden="true"></i></button>
	
	<div id="site-toc">
		<input id="search-input" class="search-input" type="search" placeholder="按回车全站搜索">
		<div id="tree">
			

			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										OS
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Linux
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										Perf
									</a>
									
							<ul>
								<li class="file">
									<a href="/2022/10/13/OS/Linux/Perf/perf_hardware2/">
                     
										    PERF EVENT 硬件篇续篇
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2022/10/13/OS/Linux/Perf/perf_hardware/">
                     
										    PERF EVENT 硬件篇
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file active">
									<a href="/2022/10/13/OS/Linux/Perf/perf_ipc/">
                     
										    Perf IPC以及CPU利用率
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2022/10/13/OS/Linux/Perf/perf_kernel/">
                     
										    PERF EVENT 内核篇
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2022/10/13/ebook/">
                     
										    Ebook
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
		</div>
	</div>
</div>

	<!-- 引入正文 -->
	<div id="content">
		<h1 id="article-title">
	Perf IPC以及CPU利用率
</h1>
<div class="article-meta">
	
	<span>Leah Ge</span>
	<span>2022-10-13 20:30:59</span>
		<div id="article-categories">
    
		<span>Categories：</span>
            
    

    
		<span>Tags：</span>
            
    
		</div>

</div>

<div id="article-content">
	<h1 id="CPU-流水线工作原理"><a href="#CPU-流水线工作原理" class="headerlink" title="CPU 流水线工作原理"></a>CPU 流水线工作原理</h1><p><a target="_blank" rel="noopener" href="https://plantegg.github.io/2020/05/31/Perf%20IPC%E4%BB%A5%E5%8F%8ACPU%E5%88%A9%E7%94%A8%E7%8E%87/">https://plantegg.github.io/2020/05/31/Perf%20IPC%E4%BB%A5%E5%8F%8ACPU%E5%88%A9%E7%94%A8%E7%8E%87/</a><br>cycles：CPU时钟周期。CPU从它的指令集(instruction set)中选择指令执行。一个指令包含以下的步骤，每个步骤由CPU的一个叫做功能单元(functional unit)的组件来进行处理，每个步骤的执行都至少需要花费一个时钟周期。</p>
<p>指令读取(instruction fetch， IF)</p>
<p>指令解码(instruction decode， ID)</p>
<p>执行(execute， EXE)</p>
<p>内存访问(memory access，MEM)</p>
<p>寄存器回写(register write-back， WB)</p>
<p>五个步骤只能串行，但是可以做成pipeline提升效率，也就是第一个指令做第二步的时候，指令读取单元可以去读取下一个指令了，如果有一个指令慢就会造成stall，也就是pipeline有地方卡壳了。<br>另外cpu可以同时有多条pipeline，这也就是理论上最大的IPC.<br>stalled-cycles，则是指令管道未能按理想状态发挥并行作用，发生停滞的时钟周期。stalled-cycles-frontend指指令读取或解码的指令步骤，而stalled-cycles-backend则是指令执行步骤。第二列中的cycles idle其实意思跟stalled是一样的，由于指令执行停滞了，所以指令管道也就空闲了，千万不要误解为CPU的空闲率。这个数值是由stalled-cycles-frontend或stalled-cycles-backend除以上面的cycles得出的</p>
<p>Back-end Bound<br>Back-end Bound 分为 Memory Bound 和 Core Bound，通过在每个周期内基于执行单元的占用情况来分析 Back-end 停顿。为了达到尽可能大的 IPC，需要使得执行单元保持繁忙。例如，在一个有4个 slot 的机器中，如果在稳定状态下只能执行三个或更少的 uOps，就不能达到最佳状态，即 IPC 等于4。这些次优周期称为 Execution Stalls。</p>
<p>非流水线：</p>
<p>对于非流水计算机而言，上一条指令的 5 个子过程全部执行完毕后才能开始下一条指令，每隔 5 个时 钟周期才有一个输出结果。因此，图3中用了 15 个时钟周期才完成 3 条指令，每条指令平均用时 5 个时钟周期。 非流水线工作方式的控制比较简单，但部件的利用率较低，系统工作速度较慢。</p>
<p>毫无疑问，非流水线效率很低下，5个单元同时只能有一个单元工作，每隔 5 个时 钟周期才有一个输出结果。每条指令用时5个时间周期。</p>
<p>标量流水线, 标量（Scalar）流水计算机是只有一条指令流水线的计算机:</p>
<p>对标量流水计算机而言，上一条指令与下一条指令的 5 个子过程在时间上可以重叠执行，当流水线满 载时，每一个时钟周期就可以输出一个结果。因此，图中仅用了 9 个时钟周期就完成了 5 条指令，每条指令平均用时 1.8 个时钟周期。</p>
<p>采用标量流水线工作方式，虽然每条指令的执行时间并未缩短，但 CPU 运行指令的总体速度却能成倍 提高。当然，作为速度提高的代价，需要增加部分硬件才能实现标量流水。</p>
<p>超标量流水线：所谓超标量（Superscalar）流 水计算机，是指它具有两条以上的指令流水线</p>
<p>当流水线满载时，每一个时钟周期可以执行 2 条以上的指令。图中仅用了 9 个时钟周期就完成了 10 条指令，每条指令平均用时 0.9 个时钟周期。 超标量流水计算机是时间并行技术和空间并行技术的综合应用。</p>
<p>在流水计算机中，指令的处理是重叠进行的，前一条指令还没有结束，第二、三条指令就陆续开始工 作。由于多条指令的重叠处理，当后继指令所需的操作数刚好是前一指令的运算结果时，便发生数据相关冲突。由于这两条指令的执行顺序直接影响到操作数读取的内容，必须等前一条指令执行完毕后才能执行后一条指令。</p>
<p>OoOE— Out-of-Order Execution 乱序执行也是在 Pentium Pro 开始引入的，它有些类似于多线程的概念。乱序执行是为了直接提升 ILP(Instruction Level Parallelism)指令级并行化的设计，在多个执行单元的超标量设计当中，一系列的执行单元可以同时运行一些没有数据关联性的若干指令，只有需要等待其他指令运算结果的数据会按照顺序执行，从而总体提升了运行效率。乱序执行引擎是一个很重要的部分，需要进行复杂的调度管理。</p>
<p>每一个功能单元的流水线的长度是不同的。事实上，不同的功能单元的流水线长度本来就不一样。我们平时所说的 14 级流水线，指的通常是进行整数计算指令的流水线长度。如果是浮点数运算，实际的流水线长度则会更长一些。</p>
<p>指令缓存（Instruction Cache）和数据缓存（Data Cache）<br>在第 1 条指令执行到访存（MEM）阶段的时候，流水线里的第 4 条指令，在执行取指令（Fetch）的操作。访存和取指令，都要进行内存数据的读取。我们的内存，只有一个地址译码器的作为地址输入，那就只能在一个时钟周期里面读取一条数据，没办法同时执行第 1 条指令的读取内存数据和第 4 条指令的读取指令代码。</p>
<p>把内存拆成两部分的解决方案，在计算机体系结构里叫作哈佛架构（Harvard Architecture），来自哈佛大学设计Mark I 型计算机时候的设计。我们今天使用的 CPU，仍然是冯·诺依曼体系结构的，并没有把内存拆成程序内存和数据内存这两部分。因为如果那样拆的话，对程序指令和数据需要的内存空间，我们就没有办法根据实际的应用去动态分配了。虽然解决了资源冲突的问题，但是也失去了灵活性。</p>
<p>在流水线产生依赖的时候必须pipeline stall，也就是让依赖的指令执行NOP。</p>
<p>每个指令需要的cycle<br>Intel xeon</p>
<p>perf 使用<br>sudo perf record -g -a -e skb:kfree_skb &#x2F;&#x2F;perf 记录丢包调用栈 然后sudo perf script 查看 （网络报文被丢弃时会调用该函数kfree_skb）<br>perf record -e ‘skb:consume_skb’ -ag  &#x2F;&#x2F;记录网络消耗<br>perf probe –add tcp_sendmsg &#x2F;&#x2F;增加监听probe  perf record -e probe:tcp_sendmsg -aR sleep 1<br>sudo perf sched record – sleep 1 &#x2F;&#x2F;记录cpu调度的延时<br>sudo perf sched latency &#x2F;&#x2F;查看<br>可以通过perf看到cpu的使用情况：</p>
<p>$sudo perf stat -a – sleep 10</p>
<p>Performance counter stats for ‘system wide’:</p>
<p> 239866.330098      task-clock (msec)         #   23.985 CPUs utilized    &#x2F;10*1000        (100.00%)<br>        45,709      context-switches          #    0.191 K&#x2F;sec                    (100.00%)<br>         1,715      cpu-migrations            #    0.007 K&#x2F;sec                    (100.00%)<br>        79,586      page-faults               #    0.332 K&#x2F;sec<br> 3,488,525,170      cycles                    #    0.015 GHz                      (83.34%)<br> 9,708,140,897      stalled-cycles-frontend   #  278.29% &#x2F;cycles frontend cycles idle     (83.34%)<br> 9,314,891,615      stalled-cycles-backend    #  267.02% &#x2F;cycles backend  cycles idle     (66.68%)<br> 2,292,955,367      instructions              #    0.66  insns per cycle  insn&#x2F;cycles<br>                                             #    4.23  stalled cycles per insn stalled-cycles-frontend&#x2F;insn (83.34%)<br>   447,584,805      branches                  #    1.866 M&#x2F;sec                    (83.33%)<br>     8,470,791      branch-misses             #    1.89% of all branches          (83.33%)</p>
<p>IPC测试<br>实际运行的时候增加如下nop到100个以上</p>
<p>void main() {</p>
<pre><code>while(1) &#123;
     __asm__ (&quot;nop\n\t&quot;
             &quot;nop\n\t&quot;
             &quot;nop&quot;);
&#125;
</code></pre>
<p>}<br>鲲鹏920运行，ipc是指每个core的IPC，如果同时运行两个如上测试程序，每个程序的IPC都是3.99</p>
<p>#perf stat – .&#x2F;nop.out<br>failed to read counter branches</p>
<p> Performance counter stats for ‘.&#x2F;nop.out’:</p>
<pre><code>   8826.948260      task-clock (msec)         #    1.000 CPUs utilized
             8      context-switches          #    0.001 K/sec
             0      cpu-migrations            #    0.000 K/sec
            37      page-faults               #    0.004 K/sec
22,949,862,030      cycles                    #    2.600 GHz
     2,099,719      stalled-cycles-frontend   #    0.01% frontend cycles idle
    18,859,839      stalled-cycles-backend    #    0.08% backend  cycles idle
91,465,043,922      instructions              #    3.99  insns per cycle
                                              #    0.00  stalled cycles per insn
</code></pre>
<p>   <not supported>      branches<br>            33,262      branch-misses             #    0.00% of all branches</p>
<pre><code>   8.827886000 seconds time elapsed
</code></pre>
<p>intel X86 8260</p>
<p>#perf stat – .&#x2F;nop.out</p>
<p> Performance counter stats for ‘.&#x2F;nop.out’:</p>
<pre><code>  65061.160345      task-clock (msec)         #    1.001 CPUs utilized
            46      context-switches          #    0.001 K/sec
            92      cpu-migrations            #    0.001 K/sec
           108      page-faults               #    0.002 K/sec
</code></pre>
<p>   155,659,827,263      cycles                    #    2.393 GHz<br>   <not supported>      stalled-cycles-frontend<br>   <not supported>      stalled-cycles-backend<br>   603,247,401,995      instructions              #    3.88  insns per cycle<br>     4,742,051,659      branches                  #   72.886 M&#x2F;sec<br>         1,799,428      branch-misses             #    0.04% of all branches</p>
<pre><code>  65.012821629 seconds time elapsed
</code></pre>
<p>这两块CPU理论IPC最大值都是4，实际x86离理论值更远一些. 增加while循环中的nop数量（从132增加到432个）IPC能提升到3.92</p>
<p>IPC和超线程的关系<br>IPC 和一个core上运行多少个进程没有关系。实际测试将两个nop绑定到一个core上，IPC不变, 因为IPC就是从core里面取到的，不针对具体进程。但是如果是两个进程绑定到一个物理core以及对应的超线程core上那么IPC就会减半。如果程序是IO bound（比如需要频繁读写内存）首先IPC远远低于理论值4的，这个时候超线程同时工作的话IPC基本能翻倍</p>
<p>对应的CPU使用率, 两个进程的CPU使用率是200%，实际产出IPC是2.1+1.64&#x3D;3.75，比单个进程的IPC为3.92小多了。而单个进程CPU使用率才100%</p>
<p>以上测试CPU为Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz (Thread(s) per core: 2)</p>
<p>perf 和火焰图<br>调用 perf record 采样几秒钟，一般需要加 -g 参数，也就是 call-graph，还需要抓取函数的调用关系。在多核的机器上，还要记得加上 -a 参数，保证获取所有 CPU Core 上的函数运行情况。至于采样数据的多少，在讲解 perf 概念的时候说过，我们可以用 -c 或者 -F 参数来控制。</p>
<p>   83  07&#x2F;08&#x2F;19 13:56:26 sudo perf record -ag -p 4759<br>   84  07&#x2F;08&#x2F;19 13:56:50 ls &#x2F;tmp&#x2F;<br>   85  07&#x2F;08&#x2F;19 13:57:06 history |tail -16<br>   86  07&#x2F;08&#x2F;19 13:57:20 sudo chmod 777 perf.data<br>   87  07&#x2F;08&#x2F;19 13:57:33 perf script &gt;out.perf<br>   88  07&#x2F;08&#x2F;19 13:59:24 ~&#x2F;tools&#x2F;FlameGraph-master&#x2F;.&#x2F;stackcollapse-perf.pl ~&#x2F;out.perf &gt;out.folded<br>   89  07&#x2F;08&#x2F;19 14:01:01 ~&#x2F;tools&#x2F;FlameGraph-master&#x2F;flamegraph.pl out.folded &gt; kernel-perf.svg<br>   90  07&#x2F;08&#x2F;19 14:01:07 ls -lh<br>   91  07&#x2F;08&#x2F;19 14:03:33 history</p>
<p>$ sudo perf record -F 99 -a -g – sleep 60 &#x2F;&#x2F;-F 99 指采样每秒钟做 99 次<br>　　执行这个命令将生成一个 perf.data 文件：</p>
<p>执行sudo perf report -n可以生成报告的预览。<br>执行sudo perf report -n –stdio可以生成一个详细的报告。<br>执行sudo perf script可以 dump 出 perf.data 的内容。</p>
<h1 id="折叠调用栈"><a href="#折叠调用栈" class="headerlink" title="折叠调用栈"></a>折叠调用栈</h1><p>$ FlameGraph&#x2F;stackcollapse-perf.pl out.perf &gt; out.folded</p>
<h1 id="生成火焰图"><a href="#生成火焰图" class="headerlink" title="生成火焰图"></a>生成火焰图</h1><p>$ FlameGraph&#x2F;flamegraph.pl out.folded &gt; out.svg<br>ECS和perf<br>在ECS会采集不到 cycles等，cpu-clock、page-faults都是内核中的软事件，cycles&#x2F;instructions得采集cpu的PMU数据，ECS采集不到这些PMU数据。</p>
<p>CPU cache</p>
<p>查看cpu cache数据</p>
<p>cat &#x2F;proc&#x2F;cpuinfo |grep -i cache</p>
<p>如下 Linux getconf 命令的输出，除了 _LINESIZE 指示了系统的 Cache Line 的大小是 64 字节外，还给出了 Cache 类别，大小。 其中 _ASSOC 则指示了该 Cache 是几路关联 (Way Associative) 的。</p>
<p>$sudo getconf -a |grep CACHE<br>LEVEL1_ICACHE_SIZE                 32768<br>LEVEL1_ICACHE_ASSOC                8<br>LEVEL1_ICACHE_LINESIZE             64<br>LEVEL1_DCACHE_SIZE                 32768<br>LEVEL1_DCACHE_ASSOC                8<br>LEVEL1_DCACHE_LINESIZE             64<br>LEVEL2_CACHE_SIZE                  262144<br>LEVEL2_CACHE_ASSOC                 4<br>LEVEL2_CACHE_LINESIZE              64<br>LEVEL3_CACHE_SIZE                  3145728<br>LEVEL3_CACHE_ASSOC                 12<br>LEVEL3_CACHE_LINESIZE              64<br>LEVEL4_CACHE_SIZE                  0<br>LEVEL4_CACHE_ASSOC                 0<br>LEVEL4_CACHE_LINESIZE              0<br>Socket、核<br>一个Socket理解一个CPU，一个CPU又可以是多核的</p>
<p>超线程（Hyperthreading，HT）<br>一个核还可以进一步分成几个逻辑核，来执行多个控制流程，这样可以进一步提高并行程度，这一技术就叫超线程，有时叫做 simultaneous multi-threading（SMT）。</p>
<p>超线程技术主要的出发点是，当处理器在运行一个线程，执行指令代码时，很多时候处理器并不会使用到全部的计算能力，部分计算能力就会处于空闲状态。而超线程技术就是通过多线程来进一步“压榨”处理器。pipeline进入stalled状态就可以切到其它超线程上</p>
<p>举个例子，如果一个线程运行过程中，必须要等到一些数据加载到缓存中以后才能继续执行，此时 CPU 就可以切换到另一个线程，去执行其他指令，而不用去处于空闲状态，等待当前线程的数据加载完毕。通常，一个传统的处理器在线程之间切换，可能需要几万个时钟周期。而一个具有 HT 超线程技术的处理器只需要 1 个时钟周期。因此就大大减小了线程之间切换的成本，从而最大限度地让处理器满负荷运转。</p>
<p>ARM芯片基本不做超线程，另外请思考为什么有了应用层的多线程切换还需要CPU层面的超线程？</p>
<p>如果physical id和core id都一样的话，说明这两个core实际是一个物理core，其中一个是HT</p>
<p>测试工具 toplev<br>toplev是一个基于perf和TMAM(Top-down Microarchitecture Analysis Method)方法的应用性能分析工具。从之前的介绍文章 中可以了解到TMAM本质上是对CPU Performance Counter的整理和加工。取得Performance Counter的读数需要perf来协助，对读数的计算进而明确是Frondend bound还是Backend bound等等。</p>
<p>在最终计算之前，你大概需要做三件事：</p>
<p>明确CPU型号，因为不同的CPU，对应的PMU也不一样（依赖网络）</p>
<p>读取TMAM需要的perf event读数</p>
<p>按TMAM规定的算法计算，具体算法在这个Excel表格里</p>
<p>这三步可以自动化地由程序来做。本质上toplev就是在做这件事。</p>
<p>toplev的Github地址：<a target="_blank" rel="noopener" href="https://github.com/andikleen/pmu-tools">https://github.com/andikleen/pmu-tools</a></p>
<p>另外补充一下，TMAM作为一种Top-down方法，它一定是分级的。通过上一级的结果下钻，最终定位性能瓶颈。那么toplev在执行的时候，也一定是包含这个“等级”概念的。</p>
<p>pmu-tools 的工具在第一次运行的时会通过 event_download.py 把本机环境的 PMU 映射表自动下载下来, 但是前提是你的机器能正常连接 01.day 的网络. 很抱歉我司内部的服务器都是不行的, 因此 pmu-tools 也提供了手动下载的方式.</p>
<p>因此当我们的环境根本无法连接外部网络的时候, 我们只能通过其他机器下载实际目标环境的事件映射表下载到另一个系统上, 有点交叉编译的意思.</p>
<p>首先获取目标机器的 CPU 型号<br>printf “GenuineIntel-6-%X\n” $(awk ‘&#x2F;model\s+:&#x2F; { print $3 ; exit } ‘ &#x2F;proc&#x2F;cpuinfo )<br>cpu的型号信息是由 vendor_id&#x2F;cpu_family&#x2F;model&#x2F;stepping 等几个标记的.</p>
<p>他其实标记了当前 CPU 是哪个系列那一代的产品, 对应的就是其微架构以及版本信息.</p>
<p>注意我们使用了 %X 按照 16 进制来打印</p>
<p>注意上面的命令显示制定了 vendor_id 等信息, 因为当前服务器端的 CPU 前面基本默认是 GenuineIntel-6 等.</p>
<p>不过如果我们是其他机器, 最好查看下 cpufino 信息确认.</p>
<p>比如我这边机器的 CPU 型号为 :</p>
<p>processor       : 7<br>vendor_id       : GenuineIntel&#96;<br>cpu family      : 6<br>model           : 85<br>model name      : Intel(R) Xeon(R) Gold 6161 CPU @ 2.20GHz<br>stepping        : 4<br>microcode       : 0x1<br>对应的结果就是 GenuineIntel-6-55-4.</p>
<p>我们也可以直接用 -v 打出来 CPU 信息.</p>
<p>$ python .&#x2F;event_download.py  -v</p>
<p>My CPU GenuineIntel-6-55-4<br>TMAM(Top-down Microarchitecture Analysis Method)<br>在最近的英特尔微体系结构上，流水线的 Front-end 每个 CPU 周期（cycle）可以分配4个 uOps ，而 Back-end 可以在每个周期中退役4个 uOps。 流水线槽（pipeline slot）代表处理一个 uOp 所需的硬件资源。 TMAM 假定对于每个 CPU 核心，在每个 CPU 周期内，有4个 pipeline slot 可用，然后使用专门设计的 PMU 事件来测量这些 pipeline slot 的使用情况。在每个 CPU 周期中，pipeline slot 可以是空的或者被 uOp 填充。 如果在一个 CPU 周期内某个 pipeline slot 是空的，称之为一次停顿（stall）。如果 CPU 经常停顿，系统性能肯定是受到影响的。TMAM 的目标就是确定系统性能问题的主要瓶颈。</p>
<p>下图展示并总结了乱序执行微体系架构中自顶向下确定性能瓶颈的分类方法。这种自顶向下的分析框架的优点是一种结构化的方法，有选择地探索可能的性能瓶颈区域。 带有权重的层次化节点，使得我们能够将分析的重点放在确实重要的问题上，同时无视那些不重要的问题。</p>
<p>例如，如果应用程序性能受到指令提取问题的严重影响， TMAM 将它分类为 Front-end Bound 这个大类。 用户或者工具可以向下探索并仅聚焦在 Front-end Bound 这个分类上，直到找到导致应用程序性能瓶颈的直接原因或一类原因。</p>
<p>toplev实例<br>配置对应cpu型号的事件</p>
<p>export EVENTMAP&#x3D;&#x2F;root&#x2F;.cache&#x2F;pmu-events&#x2F;GenuineIntel-6-55-7-core.json 这样就可以了,上述资料中的两个export是可选的</p>
<h1 id="python-toplev-py-–core-C0-–no-desc-l1-taskset-c-0-bash-c-‘echo-“7-199999”-bc-gt-x2F-dev-x2F-null’"><a href="#python-toplev-py-–core-C0-–no-desc-l1-taskset-c-0-bash-c-‘echo-“7-199999”-bc-gt-x2F-dev-x2F-null’" class="headerlink" title="python toplev.py –core C0 –no-desc -l1 taskset -c 0  bash -c ‘echo “7^199999” | bc &gt; &#x2F;dev&#x2F;null’"></a>python toplev.py –core C0 –no-desc -l1 taskset -c 0  bash -c ‘echo “7^199999” | bc &gt; &#x2F;dev&#x2F;null’</h1><p>Will measure complete system.<br>Using level 1.</p>
<h1 id="4-2-full-perf-on-Intel-R-Xeon-R-Platinum-8269CY-CPU-2-50GHz-clx-x2F-skylake"><a href="#4-2-full-perf-on-Intel-R-Xeon-R-Platinum-8269CY-CPU-2-50GHz-clx-x2F-skylake" class="headerlink" title="4.2-full-perf on Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz [clx&#x2F;skylake]"></a>4.2-full-perf on Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz [clx&#x2F;skylake]</h1><p>S0-C0    BAD            Bad_Speculation  % Slots                   36.9  &lt;&#x3D;&#x3D;<br>S1-C0    BE             Backend_Bound    % Slots                   56.2  &lt;&#x3D;&#x3D;<br>Run toplev –describe Bad_Speculation^ Backend_Bound^ to get more information on bottlenecks<br>Add –nodes ‘!+Bad_Speculation*&#x2F;2,+Backend_Bound*&#x2F;2,+MUX’ for breakdown.<br>Linux 调度策略<br>Linux kernel支持两种实时(real-time)调度策略(scheduling policy)：SCHED_FIFO和SCHED_RR</p>
<p>&#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;sched_rt_period_us</p>
<p>缺省值是1,000,000 μs (1秒)，表示实时进程的运行粒度为1秒。（注：修改这个参数请谨慎，太大或太小都可能带来问题）。</p>
<p>&#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;sched_rt_runtime_us</p>
<p>缺省值是 950,000 μs (0.95秒)，表示在1秒的运行周期里所有的实时进程一起最多可以占用0.95秒的CPU时间。</p>
<p>如果sched_rt_runtime_us&#x3D;-1，表示取消限制，意味着实时进程可以占用100%的CPU时间（慎用，有可能使系统失去控制）。</p>
<p>所以，Linux kernel默认情况下保证了普通进程无论如何都可以得到5%的CPU时间，尽管系统可能会慢如蜗牛，但管理员仍然可以利用这5%的时间设法恢复系统，比如停掉失控的实时进程，或者给自己的shell进程赋予更高的实时优先级以便执行管理任务，等等。</p>
<p>进程自愿切换(Voluntary)和强制切换(Involuntary)的次数被统计在 &#x2F;proc&#x2F;&#x2F;status 中，其中voluntary_ctxt_switches表示自愿切换的次数，nonvoluntary_ctxt_switches表示强制切换的次数，两者都是自进程启动以来的累计值。 或pidstat -w 1 来统计 <a target="_blank" rel="noopener" href="http://linuxperf.com/?cat=10">http://linuxperf.com/?cat=10</a></p>
<p>自愿切换发生的时候，进程不再处于运行状态，比如由于等待IO而阻塞(TASK_UNINTERRUPTIBLE)，或者因等待资源和特定事件而休眠(TASK_INTERRUPTIBLE)，又或者被debug&#x2F;trace设置为TASK_STOPPED&#x2F;TASK_TRACED状态；</p>
<p>强制切换发生的时候，进程仍然处于运行状态(TASK_RUNNING)，通常是由于被优先级更高的进程抢占(preempt)，或者进程的时间片用完了</p>
<p>如果一个进程的自愿切换占多数，意味着它对CPU资源的需求不高。如果一个进程的强制切换占多数，意味着对它来说CPU资源可能是个瓶颈，这里需要排除进程频繁调用sched_yield()导致强制切换的情况</p>
<p>spinlock(自旋锁)是内核中最常见的锁，它的特点是：等待锁的过程中不休眠，而是占着CPU空转，优点是避免了上下文切换的开销，缺点是该CPU空转属于浪费, 同时还有可能导致cache ping-pong，spinlock适合用来保护快进快出的临界区。持有spinlock的CPU不能被抢占，持有spinlock的代码不能休眠 <a target="_blank" rel="noopener" href="http://linuxperf.com/?p=138">http://linuxperf.com/?p=138</a></p>
<p>从操作系统的角度讲，os会维护一个ready queue（就绪的线程队列）。并且在某一时刻cpu只为ready queue中位于队列头部的线程服务。</p>
<p>sleep()使当前线程进入停滞状态，所以执行sleep()的线程在指定的时间内肯定不会执行；yield()只是使当前线程重新回到可执行状态，所以执行yield()的线程有可能在进入到可执行状态后马上又被执行。</p>
<p>NMI(non-maskable interrupt)，就是不可屏蔽的中断. NMI通常用于通知操作系统发生了无法恢复的硬件错误，也可以用于系统调试与采样，大多数服务器还提供了人工触发NMI的接口，比如NMI按钮或者iLO命令等。</p>
<p><a target="_blank" rel="noopener" href="http://cenalulu.github.io/linux/numa/">http://cenalulu.github.io/linux/numa/</a> numa原理和优缺点案例讲解</p>
<p>正在运行中的用户程序被中断之后，必须等到中断处理例程完成之后才能恢复运行，在此期间即使其它CPU是空闲的也不能换个CPU继续运行，就像被中断牢牢钉在了当前的CPU上，动弹不得，中断处理需要多长时间，用户进程就被冻结多长时间。</p>
<p>Linux kernel把中断分为两部分：hard IRQ和soft IRQ，hard IRQ只处理中断最基本的部分，保证迅速响应，尽量在最短的时间里完成，把相对耗时的工作量留给soft IRQ；soft IRQ可以被hard IRQ中断，如果soft IRQ运行时间过长，也可能会被交给内核线程ksoftirqd去继续完成。<br><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/AzcB1DwqRCoiofOOI88T9Q">https://mp.weixin.qq.com/s/AzcB1DwqRCoiofOOI88T9Q</a> softirq导致一路CPU使用过高，其它CPU还是闲置，整个系统比较慢</p>
<p>Linux的进程调度有一个不太为人熟知的特性，叫做wakeup affinity，它的初衷是这样的：如果两个进程频繁互动，那么它们很有可能共享同样的数据，把它们放到亲缘性更近的scheduling domain有助于提高缓存和内存的访问性能，所以当一个进程唤醒另一个的时候，被唤醒的进程可能会被放到相同的CPU core或者相同的NUMA节点上。这个特性缺省是打开的，它有时候很有用，但有时候却对性能有伤害作用。设想这样一个应用场景：一个主进程给成百上千个辅进程派发任务，这成百上千个辅进程被唤醒后被安排到与主进程相同的CPU core或者NUMA节点上，就会导致负载严重失衡，CPU忙的忙死、闲的闲死，造成性能下降。<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/DG1v8cUjcXpa0x2uvrRytA">https://mp.weixin.qq.com/s/DG1v8cUjcXpa0x2uvrRytA</a></p>
<p><a target="_blank" rel="noopener" href="http://linuxperf.com/?p=197">http://linuxperf.com/?p=197</a></p>
<p>参考资料<br>perf详解</p>
<p>CPU体系结构</p>
<p>震惊，用了这么多年的 CPU 利用率，其实是错的cpu占用不代表在做事情，可能是stalled，也就是流水线卡顿，但是cpu占用了，实际没事情做。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzUxNjE3MTcwMg==&amp;mid=2247483755&amp;idx=1&amp;sn=5324f7e46c91739b566dfc1d0847fc4a&amp;chksm=f9aa33b2ceddbaa478729383cac89967cc515bafa472001adc4ad42fb37e3ce473eddc3b591a&amp;mpshare=1&amp;scene=1&amp;srcid=0127mp3WJ6Kd1UOQISFg3SIC#rd">https://mp.weixin.qq.com/s?__biz=MzUxNjE3MTcwMg==&amp;mid=2247483755&amp;idx=1&amp;sn=5324f7e46c91739b566dfc1d0847fc4a&amp;chksm=f9aa33b2ceddbaa478729383cac89967cc515bafa472001adc4ad42fb37e3ce473eddc3b591a&amp;mpshare=1&amp;scene=1&amp;srcid=0127mp3WJ6Kd1UOQISFg3SIC#rd</a></p>
<p><a target="_blank" rel="noopener" href="https://kernel.taobao.org/2019/03/Top-down-Microarchitecture-Analysis-Method/">https://kernel.taobao.org/2019/03/Top-down-Microarchitecture-Analysis-Method/</a></p>

</div>


    <div class="post-guide">
        <div class="item left">
            
              <a href="/2022/10/13/ebook/">
                  <i class="fa fa-angle-left" aria-hidden="true"></i>
                  Ebook
              </a>
            
        </div>
        <div class="item right">
            
              <a href="/2022/10/13/OS/Linux/Perf/perf_kernel/">
                PERF EVENT 内核篇
                <i class="fa fa-angle-right" aria-hidden="true"></i>
              </a>
            
        </div>
    </div>




<script>
	
	
</script>

	</div>
	<div id="footer">
	<p>
	©2022-<span id="footerYear"></span> 
	<a href="/">Leah Ge</a> 
	
	
	<br>
	</p>
</div>
<script type="text/javascript"> 
	document.getElementById('footerYear').innerHTML = new Date().getFullYear() + '';
</script>

	<button id="totop-toggle" class="toggle"><i class="fa fa-angle-double-up" aria-hidden="true"></i></button>
</body>
</html>